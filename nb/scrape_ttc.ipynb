{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Newsletters from The Talon Conspiracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.selenium_resource import SeleniumResource\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import (\n",
    "    StaleElementReferenceException,\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    ")\n",
    "from urllib.parse import urlparse\n",
    "from typing import List, Optional\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "#  regular imports\n",
    "# Add path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IssuuItem:\n",
    "    issuu_name: str\n",
    "    issuu_url: str\n",
    "    issuu_img_src: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTCContent:\n",
    "    ttc_content_title: str\n",
    "    ttc_items: List[IssuuItem]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTCTag:\n",
    "    \"The Talon Conpiracy - Items by Tag\"\n",
    "    ttc_accessible_name: str\n",
    "    ttc_tag_name: str\n",
    "    ttc_tag_url: str\n",
    "    ttc_tag_content: List[TTCContent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "\n",
    "def setup_logging(log_level=logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure logging to both file and console with rotation and formatting.\n",
    "\n",
    "    Args:\n",
    "        log_level: The logging level to use (default: logging.INFO)\n",
    "    \"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    log_file = os.path.join(os.getcwd(), 'ttc_scrape.log')\n",
    "\n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # Configure root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(log_level)\n",
    "\n",
    "    # Clear any existing handlers\n",
    "    root_logger.handlers = []\n",
    "\n",
    "    # Create rotating file handler (10MB max size, keep 5 backup files)\n",
    "    file_handler = RotatingFileHandler(\n",
    "        filename=log_file,\n",
    "        maxBytes=10*1024*1024,  # 10MB\n",
    "        backupCount=5,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(log_level)\n",
    "\n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    console_handler.setLevel(log_level)\n",
    "\n",
    "    # Add handlers to root logger\n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "    # Log system info at startup\n",
    "    logger = logging.getLogger(\"TTC Scrape Logger\")\n",
    "    logger.info('='*50)\n",
    "    logger.info(f'Logging initiated at {\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "    logger.info(f'Log file location: {log_file}')\n",
    "    logger.info('='*50)\n",
    "    return logger\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, max_retries: int = 3, timeout: int = 10):\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.logger = setup_logging()\n",
    "\n",
    "    def wait_and_find_element(\n",
    "            self,\n",
    "            driver,\n",
    "            by: By,\n",
    "            value: str,\n",
    "            timeout: Optional[int] = None,\n",
    "    ) -> Optional[any]:\n",
    "        \"\"\"Safely wait for and find an element with retries.\"\"\"\n",
    "        timeout = timeout or self.timeout\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                return WebDriverWait(driver, timeout).until(\n",
    "                    EC.presence_of_element_located((by, value))\n",
    "                )\n",
    "            except StaleElementReferenceException:\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(1)\n",
    "            except TimeoutException:\n",
    "                self.logger.warning(\n",
    "                    f\"Timeout waiting for element {by}={value}\")\n",
    "                return None\n",
    "\n",
    "    def wait_and_find_elements(\n",
    "        self,\n",
    "        driver,\n",
    "        by: By,\n",
    "        value: str,\n",
    "        timeout: Optional[int] = None,\n",
    "    ) -> List[any]:\n",
    "        \"\"\"Safely wait for and find elements with retries.\"\"\"\n",
    "        timeout = timeout or self.timeout\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                elements = WebDriverWait(driver, timeout).until(\n",
    "                    EC.presence_of_all_elements_located((by, value))\n",
    "                )\n",
    "                return elements\n",
    "            except StaleElementReferenceException:\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(1)\n",
    "            except TimeoutException:\n",
    "                self.logger.warning(\n",
    "                    f\"Timeout waiting for elements {by}={value}\")\n",
    "                return []\n",
    "\n",
    "    def get_attribute_safely(self, element, attribute: str) -> Optional[str]:\n",
    "        \"\"\"Safely get an attribute from an element with retries.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                return element.get_attribute(attribute)\n",
    "            except StaleElementReferenceException:\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    return None\n",
    "                time.sleep(1)\n",
    "\n",
    "\n",
    "def scrape_ttc_content():\n",
    "    ttc_url = \"https://thetalonconspiracy.com/\"\n",
    "    client = SeleniumResource()\n",
    "    scraper = WebScraper()\n",
    "    ttc_tags = []\n",
    "\n",
    "    try:\n",
    "        client.setup_for_execution()\n",
    "        driver = client.driver\n",
    "        driver.get(ttc_url)\n",
    "\n",
    "        # Main scraping loop\n",
    "        while True:\n",
    "            # Wait for sidebar and get links\n",
    "            sidebar_tag_links = scraper.wait_and_find_elements(\n",
    "                driver,\n",
    "                By.XPATH,\n",
    "                \"//div[@id='sidebarleft']//li//a[@href]\"\n",
    "            )\n",
    "            if not sidebar_tag_links:\n",
    "                break\n",
    "\n",
    "            # Process each tag link\n",
    "            for link in sidebar_tag_links:\n",
    "                ttc_accessible_name = link.accessible_name\n",
    "                tag_url = scraper.get_attribute_safely(link, 'href')\n",
    "                if not tag_url:\n",
    "                    continue\n",
    "\n",
    "                parsed_url = urlparse(tag_url).path\n",
    "                tag_name = parsed_url.strip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "                # Navigate to tag page\n",
    "                driver.get(tag_url)\n",
    "\n",
    "                # Get content results\n",
    "                content_results = scraper.wait_and_find_elements(\n",
    "                    driver,\n",
    "                    By.CLASS_NAME,\n",
    "                    \"results_content\"\n",
    "                )\n",
    "\n",
    "                ttc_content = []\n",
    "                for idx, result in enumerate(content_results):\n",
    "                    try:\n",
    "                        # Get the Category type. Include only periodicals.\n",
    "\n",
    "                        category_labels = [category.text for category in result.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"h3 a[rel]\"\n",
    "                        )]\n",
    "\n",
    "                        if \"PERIODICALS\" not in category_labels:\n",
    "                            continue\n",
    "\n",
    "                        # Get title from the current result element\n",
    "                        result_title = result.find_element(By.TAG_NAME, \"h1\")\n",
    "                        if not result_title:\n",
    "                            logging.warning(\n",
    "                                f\"No title found for result {idx + 1}\")\n",
    "                            continue\n",
    "\n",
    "                        title_text = result_title.text\n",
    "                        logging.info(f\"Found title for result {\n",
    "                                     idx + 1}: {title_text}\")\n",
    "\n",
    "                        # Get links with images\n",
    "                        links_with_images = result.find_elements(\n",
    "                            By.CSS_SELECTOR,\n",
    "                            \"a:has(img)\",\n",
    "                        )\n",
    "                        if not links_with_images:\n",
    "                            logging.warning(f\"No links with images {idx + 1}\")\n",
    "                            continue\n",
    "\n",
    "                        ttc_issuus = []\n",
    "                        for link_img in links_with_images:\n",
    "                            issuu_href = scraper.get_attribute_safely(\n",
    "                                link_img, \"href\")\n",
    "                            img_element = link_img.find_element(\n",
    "                                By.TAG_NAME,\n",
    "                                \"img\",\n",
    "                            )\n",
    "                            if not img_element:\n",
    "                                continue\n",
    "\n",
    "                            issuu_img_src = scraper.get_attribute_safely(\n",
    "                                img_element, \"src\")\n",
    "                            if not issuu_href or not issuu_img_src:\n",
    "                                continue\n",
    "\n",
    "                            issu_name = urlparse(issuu_img_src).path.strip(\n",
    "                                \"/\").split(\"/\")[-1]\n",
    "                            issuu_item = IssuuItem(\n",
    "                                issuu_name=issu_name,\n",
    "                                issuu_url=issuu_href,\n",
    "                                issuu_img_src=issuu_img_src\n",
    "                            )\n",
    "                            ttc_issuus.append(issuu_item)\n",
    "\n",
    "                        ttc_content_result = TTCContent(\n",
    "                            ttc_content_title=result_title.text,\n",
    "                            ttc_items=ttc_issuus.copy()\n",
    "                        )\n",
    "                        ttc_content.append(ttc_content_result)\n",
    "\n",
    "                    except StaleElementReferenceException:\n",
    "                        continue\n",
    "                    except NoSuchElementException:\n",
    "                        logging.warning(f\"Missing h1 tag in result {idx + 1}\")\n",
    "                        continue\n",
    "                    except StaleElementReferenceException:\n",
    "                        logging.warning(\n",
    "                            f\"Result {idx + 1} became stale, skipping\")\n",
    "                        continue\n",
    "\n",
    "                ttc_tag = TTCTag(\n",
    "                    ttc_accessible_name=ttc_accessible_name,\n",
    "                    ttc_tag_name=tag_name,\n",
    "                    ttc_tag_url=tag_url,\n",
    "                    ttc_tag_content=ttc_content.copy()\n",
    "                )\n",
    "                ttc_tags.append(ttc_tag)\n",
    "\n",
    "                # Navigate back\n",
    "                driver.get(ttc_url)\n",
    "\n",
    "\n",
    "            break  # Exit the main loop after processing all tags\n",
    "\n",
    "        return ttc_tags\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        return []\n",
    "    finally:\n",
    "        client.teardown_after_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-27 05:05:03] INFO [TTC Scrape Logger:49] ==================================================\n",
      "[2024-11-27 05:05:03] INFO [TTC Scrape Logger:50] Logging initiated at 2024-11-27 05:05:03\n",
      "[2024-11-27 05:05:03] INFO [TTC Scrape Logger:52] Log file location: /app/nb/ttc_scrape.log\n",
      "[2024-11-27 05:05:03] INFO [TTC Scrape Logger:53] ==================================================\n",
      "[2024-11-27 05:07:29] INFO [root:184] Found title for result 1: NO COMPROMISE #15-17\n",
      "[2024-11-27 05:20:54] ERROR [root:253] An error occurred: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=131.0.6778.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "#0 0x57b53c77f31a <unknown>\n",
      "#1 0x57b53c2956e0 <unknown>\n",
      "#2 0x57b53c2a3881 <unknown>\n",
      "#3 0x57b53c2ea936 <unknown>\n",
      "#4 0x57b53c2df252 <unknown>\n",
      "#5 0x57b53c308462 <unknown>\n",
      "#6 0x57b53c2d8a18 <unknown>\n",
      "#7 0x57b53c30862e <unknown>\n",
      "#8 0x57b53c326ed7 <unknown>\n",
      "#9 0x57b53c308203 <unknown>\n",
      "#10 0x57b53c2d6cc0 <unknown>\n",
      "#11 0x57b53c2d7c9e <unknown>\n",
      "#12 0x57b53c74cd0b <unknown>\n",
      "#13 0x57b53c750c92 <unknown>\n",
      "#14 0x57b53c739b3c <unknown>\n",
      "#15 0x57b53c751807 <unknown>\n",
      "#16 0x57b53c71f0df <unknown>\n",
      "#17 0x57b53c76e578 <unknown>\n",
      "#18 0x57b53c76e740 <unknown>\n",
      "#19 0x57b53c77e196 <unknown>\n",
      "#20 0x7e73285161c4 <unknown>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_ttc_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
